# More numbers

Using last time info of 220MB/s for the buffered disk reads.

The number of variants will be estimated at ~3M.

We do have a problem with the TFrecord, the data is stupidly huge even if they are supposed to be
encoded in u8, indeeed teh tfrecords for NA12878 is 21GB, but we have: `2000*4*100` ~ 1MB per tensor
and we should only have ~3k variants for him.

Thus we should have only 3-5GB max, and we have up to 4 times that ! This is very worrying !

The filtered BAM weight 86MB for HG0096, assuming that it is the mean this gives us 215GB total for
all variants (150GB in practice if my memory serves me right)

We end up with 2500 * 20GB ~ 50TB, to which we have to add 3 negative sample per positive sample,
this gives us ~200TB.

The total amount to read is thus 200TB, it should take 200TB / 200MB/s ~= 277h.

|       | 10 epochs | 20 epochs |
| ----- | --------- | --------- |
| hours | 2770      | 5540      |
| days  | 115       | 230       |

Thus between 5-10d with 20 machines.
