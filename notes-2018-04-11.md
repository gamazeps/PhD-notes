# Massive Data and how to catch them

From Sunday up to Tuesday I worked on having all my data located into a single big matrix.

Extracting the reads takes about 3 hours.
Generating the tensors takes about 4 hours.

The read extraction part was done in order to use pysam and get rid of the rust code.

After that I started merging all my hdf5 files together and this is a rather long process,
the whole files weigh about 1.5TB and they only contain the positive training cases

Tensors are currently separated into 20 files (one per machine) and a long merging is running
on node1 (expected running time: 3 days).

A first test with a dummy VGG-net was launched on a compressed (lzf) file and it showed to require 2
days for a single epoch, with more that 50% of the time wasted decompressing the data (wich is a
single threaded operation).

Even if compressing is cheaper than generating anew, since it is not parralellizable, it may be
worth it to generate on the fly instead.

Olivier Griselle may have some insights on this.

## Nucleus investigation

Nucleus seems to be a concurrent interface to pysam, its efficiemcy would need to be benchmarked to
see if it is worth replacing it in my code (maybe email cory or open an issue).

## DeepVariant investigation

Looking at how deepvariant does its thing, it seems to generate the tensors on the fly.

The rows are build one by one (in C++) and then stacked (with numpy), and the image also
seems to be generated when needed, using a `data_provdier`.

This would avoid having to spend a lot of times compressing and decompressing the data.

## Data dimensions

- Storing the reference on top of the reads allow to almost halve the number of dimensions,
  however it makes comparing to the reference hard for the reads,
  A solution would be to have one channel for the CIGAR string that would allow to store that in just
  one channel.
- Encoding the base with a number instead of in one-hot is another way to reduce the dimennsion, but
  may force a larger number of layers because it would probably be recomputed by the network.
- Finding a solution for having more that one read per row would also be very good as it would help
  drastically reduce the numer of rows, however this may be tricky to keep pairs information.

## Recomb

I registered for RECOMB2018

TODO:

- remove the rust code.
- benchmark the time needed to read a batch depending on the file size.
- benchmark how long it takes to generate a batch of samples from a selected bam
- i.e. : is it worth it to preprocess and how much is it worth it ?
