# Attention ! Attention !

Today is focused on the following tasks:

- Better understanding of attention models
- Polish up my SV calling review
- Finish up my CNN implementation

## Attention models

Same paper as Monday, except that this time I actually managed to read it (~1h), I will probably
need to read it a second time because the first part is rather dense.

Problem statement: Current multiple object detection tasks are extremely computationaly expensive
and are done using a sliding window and a CNN, this does not scale to large images.

Proposed solution: Use a small CNN that works on `glimpses` of the image and have an RNN select the
glimpses to look at (the attention part).

The reason why they use an attention model is because most current approachs used a separately
trained sequence detector or a bottom up proposal generator and the authors wanted to improve on
that part.

*Note*: The proposal generator might be interesting in variant calling as it is currently done using
a heuristic (a good one, but a bottom-up, stateless, approach nevertheless).

The networks work as follow:

- A Glimpse networks, that takes the image and location as inputs and output a vetor representation
  of the location (3 layers CNN).
- A Reccurent network with two recurrent layers, one for the internal state and one for the
  candidate location.
- A context network used at the beginning on a downsampled version of the image to extract
  structural information (optional).
- Clasification network, a FC layer working on the hidden state of the Recurrent network.

The math behind the learning are a bit tricky for me right now, they apparentlt optimised on the
free energy (no idea what that is), and then used a few approximation to compute the gradient (some
Monte Carlo which I believe is just the reformulation of SGD and another trick for the prediction).

*Note*: The function that they optimized for was derived without using RL algorithm and yet can
still be stated as an RL problem where the RNN works as an axploration policy (there is an
hyperparameter there, the variance $\sigma$ of the gaussian they used for MC).  
That is interesting because it shows that RL can be expressed directly as a learning task.

In addition their architecture is trained for ordered sequence of object detection, this is
something we do not care for in variant calling, relaxing that constraint might give better results
for us.

### Why do we care ?

First of all the results are better than the state of the art at the time, not by much but that is
still a reason to care (3.96 vs 3.9 on street view numbers).

In my opinion the interesting part is that their architecture can work on images of all sizes, where
non retrained CNNs cannot do that. Hence it "learns" more than they do as they are able to
generalize.

Furthermore they used significantly less parameters and FLOPS than CNNs architectures (3-4x less
Flops and 2-8x less parameters). The number of parameters is independant of the size of the image
which is a great property. This also means much lower raining time (hours vs days).

*Note* they compared to regular CNNs and not to Inception, known to have much less parameters
and FLOPs, so the comparaison may not be fair.

### Plan for the future

It would be nice to read on "attention is all you need" and the Mnih paper (Reccurrent models of
visual attention) which introduces the Recurrent Attention Model (RAM).
