# Variants data

I downloaded and unzipped the data from ncbi for the SVs with

```
wget -r ftp://ftp.ncbi.nlm.nih.gov/pub/dbVar/data/Homo_sapiens/by_study/estd219_1000_Genomes_Consortium_Phase_3_Integrated_SV/vcf/ 
```

Because the vcfs from the 1000gp ftp were at best unreadable (and the ones from were at best very
hard to understand).

Some numbers:

```
➜  vcf: cat estd219_1000_Genomes_Consortium_Phase_3_Integrated_SV.GRCh37.submitted.variant_call.germline.vcf | grep -i 'NA12878' | wc -l
3265
➜  vcf: cat estd219_1000_Genomes_Consortium_Phase_3_Integrated_SV.GRCh37.submitted.variant_call.germline.vcf | wc -l                    
8812590
```

The format of the data is:

```
➜  vcf: cat estd219_1000_Genomes_Consortium_Phase_3_Integrated_SV.GRCh37.submitted.variant_call.germline.vcf | grep -i 'NA12878' | head -n 1
1       2911537 essv16945607    A       <DEL:ME:ALU>    .       .  DBVARID;CALLID=DEL_pindel_91_NA12878;SVTYPE=DEL;EXPERIMENT=9;SAMPLE=NA12878;END=2911850;REGION=esv3818169
```

Which is very good for us.

One drawback is that there may be many duplicates in these SV if there are no SNPs or indels near
them, we will have to do some form of deduplication to know the actual number of variants.

Nevertheless it is good to see that we may have a *lot* of samples to train on ! Even by making a
conservative assuption that each SV is shared by 100 person with no SNPs nearby to differentiate
them we woul have ~8Ok samples.

# Grabbing the reads

We can grab the reads for a single deletion the following way:

```
➜  vcf: cat estd219_1000_Genomes_Consortium_Phase_3_Integrated_SV.GRCh37.submitted.variant_call.germline.vcf | grep -i 'NA12878' | grep -i "<DEL>" | head -n 3
1       4204668 essv16952676    C       <DEL>   .       .  DBVARID;CALLID=DEL_pindel_129_NA12878;SVTYPE=DEL;EXPERIMENT=9;SAMPLE=NA12878;END=4204717;REGION=esv3818201

➜  vcf: samtools view NA12878.mapped.ILLUMINA.bwa.CEU.low_coverage.20121211.bam "1:4204668-4204717" | less 
```

This gives us 16 reads in SAM format with which we can easily work in pyhton (in an ugly inneficient
way but still). Note that we can parralelize this pretty fast with rust (which is my plan).

# Implementing

After uncareful consideration I decided to go with rust so that I can be sure that my implementation
is properly parallelized, the rationale is the following:

- Python is a headache and may be too slow.
- C++ is just asking for troubles, plus it would force me to play with htslib.
- Rust, well I'm rusty but should be up and running in less than a day and I have a more than decent
  knowledge on how to parralelize.
- Java is well... Java. Plus I have very little experience with its concurrency system.

It will be also a nice occasion to toy with rust-bio at some point.

Since this is only for the preprocessing, I will dump my results on simple text files in order to
keep them, thus I don't encounter any debt by using any language and I can just as well finish in
java/c++/pyhton if needed.

# Code

Just created the repository on [github](github.com/gamazeps/DeepSV).
