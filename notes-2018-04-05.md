# Getting back to it !

The last month was a slow months I don't really know why... But luckily the past week
was decent enough work-wise.

The next few notes will probably suck, because I haven't written in a while.

Some thoughts:

- The variants really look bad, I wonder if I used the wrong reference ? That is entirely possible.
- The rust code should be demolished and replaced by python using [pysam](http://pysam.readthedocs.io/en/latest/api.html).
- It might be worth it to put everything in Spark, as my distribution with shell script is bad
  to say the least.
- Nucleus by google seems like a promising stuff to look at.

For some reason my job fail from time to time and my way to keep track of which data was generated
how is completely failing, this is very bad.

The current objective is now to get a big add h5 file with all my records, in the ugly way, and then
to recode everything, so that I can parralellize model training and coding the distribution of
the code.



NO ! FUCK THAT SHIT, I'll rewrite evrything in spark in a coffee run !


# Proper data

Data for the 1000GP is [estd219](https://www.ncbi.nlm.nih.gov/dbvar/studies/estd219/).
There is also a better callset by the 1000GP [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/integrated_sv_map/)

# aftermath

The coffee run did not happen.

I however used pysam and it will probably allow me to get rid of teh whole rust part pretty quickly.
Once I get rid of the rust part, I can integrate the tensor generation and read extraction in a
single program.

Once that is done it should be easy to sparkify, the only issue left will be to access the bam files
from spark in my mappers (they can probbaly be stateful and "own" files)
