Happy new year !

# CrowdVariant

Greenside, Peyton, Justin Zook, Marc Salit, Ryan Poplin, Madeleine Cule, and Mark DePristo.
“CrowdVariant: A Crowdsourcing Approach to Classify Copy Number Variants.” BioRxiv, December 13,
2016, 093526. https://doi.org/10.1101/093526.

*Note*: First author is a PhD student at the Stanford bioinfo lab that uses deep learning (and is
part of the ENCODE project). There is also Justin M. Zook from NIST, who leads a lot of stuf in the
GIAB consortium.

This paper describes a crowdsourcing procedure for calling CNVs (but not mCNVs), that is based on
generating an image with IGV and then ask both experts and non experts to classify it as a CNV (or
non variant).

They show that non experts and experts perform extremely close to each other and that taking a
consensus vote on the predictions of the participants gives results extremely similar to those of
svviz.

The conclusion of the article summurizes their main idea quite simply, while also showing that the
rationale behing DeepSV is more than reasonable:

```
We show that individuals with no background in genomics can be trained to accurately
classify and thereby curate copy number variants. This is possible because, ultimately,
the classification of CNVs based on images of aligned NGS reads is a pattern
recognition problem, and even non-experts with limited training can excel at
recognizing these patterns
```

# DeepSV training and prediction mode

*Note*: Github does not support embedded LaTeX for now, thus this will render pootly there.
Compiling it with `pandoc` will be needed for having a proper renderring of the maths.

## Notation

Here we will denote to the _DeepSV classifier_ as $DS$, its application to a sample $DS(x) = \^{y}$.

Note that we will denote to the classifier the same way whether it is trained or untrained as it
should be obvious from the context (untrained in training, trained in prediction).

We will also refer to the data representation as _enriched pileup image (EPI)_.

## Training phase

The model is a simple classifier, it thus simply needs a set of labeled data, hopefully we can get
them on dbVar or directly from the GiaB consortium for some individuals.

### Positive labels

We generate the positive samples by reading the VCF file containing the known variants for a patient
(probably HG001 or HG002) or set of patients.

Once we know of these mutations we take the reads in the region of that variant (if the breakpoint
location is precise enough).

When we have the reads and the location we apply our transformation for having an enriched pileup
image.

We now have a set of labeled data for the positive class.

### Negative labels

For generating the negatives labels, we choose regions known to have no variants in the patients, of
varying sizes, and generate an enriched pileup image for that region.

We thus have a set of labaled data for the negative class.

*Note*: We can also use the false positives of other SV callers so that our method is trained to
classify them as true negatives, it may nevertheless be considered "cheating" to use this for the
benchmarking prooving that our method is superior, but it is still a good datapoint to give (as long
as we don't use this for claiming superiority).

### Actual training

Now that we have generated the data, we simply train our classifier with gradient descent and
backpropagation, the exact way to do that will be determined at implementation time (i.e.
regularization, fine tuning, early stopping, dropout, optimization algorithm, parameter
initilaization etc...).

## Prediction phase

For the prediction phase we generate an enriched pileup image for each candidate window and feed
that to our (trained) classifier.

The output of the classifier will be the prediction of our variant caller.

![Prediction flow](Images/DeepSV_prediction_flow.jpg "Prediction Flow")

## Discussion

We may want to try to train our classifier on various technologies and species to see if transfer
learning is possible, and if we can use it to do discovery on new species.
