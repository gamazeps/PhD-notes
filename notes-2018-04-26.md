# Meeting -- 2018/04/26

## Profiling the application

The profiling was attempted in two ways:

- Using `perf`, the UNIX tool, this profiler generated a very large file of logs (600MB) which ended
  up being unusable as it did not show how each thread worked (espcially simce the number of threads
  varies during the execution).  
  Furthermore this records function calls, which is not that interesting for us since the most
  common function call is to matrix multiplication, but it doesn't tell us at which step.
- Using TensorFlow's `TimeLine` object, this logs the time taken for each operation in tensorflow
  as well as what each thread is doing.

The second solution is exactly what we need and I realized that this is actually the canonical way
to profile TensorFlow applications.

There was however a caveat here: my code was implemented using [`keras`](https://keras.io/) which
has a very poor support for profiling (it only measure the last pass in the data). As a consequence
the metrics are not evry useful. In particular IO costs do not appear.

The solution will be to implement the deep learning pass using native tensorflow code.

## Issues with HDF5

The monothreaded reading of the data came from the software I was using: Keras.

Indeed it does not support parrallel reads to HDF5 matrices (see issue [#6298](https://github.com/keras-team/keras/issues/6298)),
there is a solution by using data generators (see issue [#1627](https://github.com/keras-team/keras/issues/1627)),
however this requires a decent anount of coding thus rendering the use of keras moot.

## HDF5 file structure

To quote the HDF5 specification [introduction](https://support.hdfgroup.org/HDF5/doc/H5.intro.html#Intro-FileTech):

> At the lowest level, as information is actually written to the disk, an HDF5 file is made up
> of the following objects:
> 
> - A super block
> - B-tree nodes (containing either symbol nodes or raw data chunks)
> - Object headers
> - Collections
> - Local heaps
> - Free space
>
> The HDF5 library uses these lower-level objects to represent the higher-level objects that are
> then presented to the user or to applications through the APIs. For instance, a group is an
> object header that contains a message that points to a local heap and to a B-tree which points
> to symbol nodes. A dataset is an object header that contains messages that describe datatype,
> space, layout, filters, external files, fill value, etc with the layout message pointing to
> either a raw data chunk or to a B-tree that points to raw data chunk

More details can be found in the [Format Specification](https://support.hdfgroup.org/HDF5/doc/H5.format.html)

In a nutshell, the actual data is split into `chunks` whose size is either automatically decided
or specified when creating the file.  
The compression is applied at the chunk level (see [HDF5 chunking specification](https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/index.html))

# Proposed solution

## Machine learning

Moving the machine learning part to tensorflow seems the best solution for the following reasons:

- Better support for profiling.
- Better support for data ingestion via its generators.
- Better tooling.
- Allows for finer tuning of IO and distribution.
- Better documentation.
- Issues are answered faster on github.

The drawbacks are the following:

- Implementation is more complex.
- Lower level programming may lead to new classes of bugs.

In my opinion the advantages outweight the drawbacks.

It is also noteworthy that most "big" applications choose to use tensorflow, even though that is a
poor argument.

## Data storage

The HDF5 storage was not to blame for the execution time, it may thus be a good idea to keep it.

However it may be noted that quite a few applications use TFRecord instead of HDF5, this format is
build specifically for tensorflow.

### Generation on-the-fly

Data generator will be needed in any case to have parrallel reads to HDF5, this generation on the
fly may thus not be costly (programming wise).

However my understanding of how deepVariant was implemented was wrong as they do not do generation
on the fly but instead preprocess the data beforehand.

In our case it may still be worth it as unprocessed data is ~100 fold lighter than processed data,
we could thus have everything fit in memory all the time.
