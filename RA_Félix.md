# 3.7 Design and optimisation for population genomics

One particular case of area where the increase in data production is the more
consequent is genomic data, indeed the amount of data produced doubles every 7
months.
Thus we want to bring the expertise from the database and big data community to
help both scale the existing algorithms and design new algorithms that are
scalable from the ground up.

Here we investigate new means to discover Copy Number Variation in the human
population using methods from the deep learning community. Indeed great success
has been shown in that area with projects such as DeepVariant, such projects
managed to considerably lower the latency for getting results (about 10 fold)
but at a higher computational cost.

As the area of population genomics is fairly new, we hope to help design a
complete framework allowing for better optimisations and integration with
database tools;

# 4.4 Genomics

As mentionned above the area of genomics experiences a massive increase in the
amount of data to be processed. Furthermore the data generated can sometimes
hard to interpret (in particular NGS data for CNV detection).

Here we are building a collaboration with the New York Genome Center for
designing new algorithms and have high hopes to have our results published in
either bioinformatics journals or big data conferences.

Our partners at the NYGC are Dr. Avinash Abhyankar, advising FÃ©lix Raimundo,
and Toby Bloom (head of the informatics at NYGC) who also works with Pr. Diao
on another big data project for genomics.
