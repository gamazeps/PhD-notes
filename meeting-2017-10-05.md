# Main topic

This week's meeting plan was to go deep into convolutional neural networks and rapidly go over
deepvariant insights.

## Topics covered

### With yanlei only

As a part of the training (on Thursday):

- Introduced linear classifiers and regression and the need for non linear transformation.
- Introduced the pattern of linear unit followed by a non linearity (called activation)
    - $z = Wx + b$
    - $a = f(z)$ 

As a part of the training (on Friday):

- Neural network as parametrized function approximator.
- Distance between functions as a loss function.
- Modular backpropagation for computing derivatives in a computational graph:
    - Each layer (index l) has an input z(l-1), an output z(l) and parameters \theta (l)
    - Each layer only needs to specify dz(l)/dz(l-1) and dz(l)/dtheta(l)
- Introduced the notion of computational graph and layers as node of the graph.

### With everyone

- The plan was not exactly followed and most of the time was spent on finishing up the analysis of
  deep variant, both insights and drawbacks.
- We decided to focus for the time being on Structural Variants calling.
- We quickly talked about RNNs (stateful neural networks).

## Topics not covered

- Convolutional neural networks.

## Remarks on the presentation

- I failed to do a good summary of last weeks meeting (on Thursday and Friday), which I will do
  from now on.
- I did not write a suamry of deep variants with schemas, which was done on Friday (attached in the
  email).

## Plan for next week

- Present the maths behind Convnets as well as some history (are you ok with that profesor ?)
- If time permits it, investigate use of RNNs for SV calling.
