# Main topic

Today's meeting was planned as a deep dive into convolutional networks and why they work
for SNPs and small indels.

Only Avinash was present so we chose to spend the time on the properties of convolutional
neural networks, their insights for SNPs and potential other interesting models for SVs

## Topics covered

- Last week's last slides on deep variant (transfer learning accross species, retraining on new
  sequencers, distribution of the analysis).
- Linear classification and its limits.
- Neural network for learning parametric non linear decision boundaries.
- Definition of loss functions for parametric optimization (finding the "right" parameters).
- Overfitting, and problems of models with many parameters
- Motivation for convolutional networks (applying the same local transformations to the whole data)
- Reducing overfiting by having less parameters allows for deeper networks
- Convolutional neural networks as feature extractors.
- Transfer learning for speeding training.
- Back to DeepVariant with this knowledge.
- Insights (DeepVariant learns to recognize mutations with no population priors).

## Topics not covered

- Deep dive in training deep neural networks.
- Training of convolutional networks with computational graph.

## Topics covered (unplained)

- Quick presentation of problems solved by RNNs: HMMs + Viterbi (working with sequence of data).
- Lack of unified methods for SVs calling.
- DeepNano succesfully calling bases.
- Overview of attention models (not in the slides).

## Remarks on the presentation

- The topic was broader than planned but gave an overview of what kind of problems deep learning is
good at solving and the sort of solutions it can bring.
- Some mention was done of the litterature review of SV calling
- Avinash seemed glad to get this overview as it will help to understand how and if deep learning is
  appropriate.
- We seemed to agree that many methods could be used for SV calling.
- DeepVariant would not be an appropriate algorithm for SV calling as it works on short windows
  (221bp), which is not sufficient for SV calling. Furthermore it works on realigned data which is
  not good for calling inversions, deletions, translocations, CNVs...
- RNNs have been mentionned as a possible solution for working on probabilistic graphs, a
  transformation of a De Bruijn graph could be interesting.
- DeepVariant currently only works on diploid data, but could be modified for other kind of ploidy
  (the assumption of diploidy is done in the candidates emission) which would be useful for chimeric
  reads in cancer cells.

## Work for next week

- The formal optimization and probabilistic methods presentation is a candidate.

# References

Avinash was interested in some readings, here are the ones that may be interesting:

- Andrej Karpathy's short introduction to recurrent neural networks and attention models: ["The
  unreasonable effectiveness of Recurrent Neural
Networks"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (2015).
- DeepNano, a success of RNNs for base calling. [Boža V, Brejová B, Vinař T. DeepNano: Deep recurrent neural networks for base calling in MinION nanopore reads. Zhi D, ed. PLoS ONE. 2017;12(6):e0178751. doi:10.1371/journal.pone.0178751.](https://www.ncbi.nlm.nih.gov/pubmed/28582401) 
- Overview of the current pipelines and algorithms used for SV calling (the one mentionning the need
  of confidence interval and ensemble methods for SV calling): [Lin K., Smit S., Bonnema G., Sanchez-Perez G., de Ridder D. (2014). Making the difference: integrating structural variation detection tools. Brief. Bioinform. 10.1093/bib/bbu047 ](https://www.ncbi.nlm.nih.gov/pubmed/25504367)

Specific part of interest in this paper:

```
The integration of several signals for SV detection as discussed
in this review is promising, but risks introducing additional
false negatives. This problem can be tackled by taking a
machine learning approach to the merge step, i.e. combining information
obtained from individual callers into a final prediction
and optimizing the parameters of this process (‘training’)
by learning from example data. A number of tools using such a
supervised learning approach have been developed, but these
are currently limited to detecting deletions and duplications
```
